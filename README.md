# ğŸ›°ï¸ SIBGRAPI2025\_SLAM

**3D Reconstruction from Monocular Images using Depth Estimation and Point Cloud Fusion**
Modular pipeline for scene reconstruction from RGB images, leveraging monocular depth inference, point cloud generation, fusion, and visualization using Open3D â€” all without ROS2.

---

## ğŸ¯ Objective

This project aims to build a robust and modular pipeline for **3D reconstruction from monocular RGB images**, including the following components:

* ğŸ“· RGB + Depth capture with RealSense (offline)
* ğŸ” Monocular depth estimation with **DepthAnythingV2**
* ğŸ” Point cloud reconstruction using **Open3D**
* ğŸ”„ Frame alignment via **ICP**
* ğŸ§© Depth map fusion (real + estimated)
* ğŸ—ºï¸ Final visualization and export of the fused 3D map

---

## ğŸ“ Project Structure

```bash
SIBGRAPI2025_slam/
â”‚
â”œâ”€â”€ checkpoints/                  # Pre-trained weights for DepthAnything
â”œâ”€â”€ datasets/                     # Input images, depth maps, and intrinsics
â”‚   â””â”€â”€ <scene>/
â”‚       â”œâ”€â”€ rgb/
â”‚       â”œâ”€â”€ depth_npy/
â”‚       â””â”€â”€ intrinsics.json
â”‚
â”œâ”€â”€ results/                      # Output organized by step
â”‚   â””â”€â”€ <scene>/
â”‚       â””â”€â”€ step_1/step_2/...step_3/
â”‚
â”œâ”€â”€ modules/                      # Modular implementation
â”‚   â”œâ”€â”€ inference/              # Monocular depth inference
â”‚   â”œâ”€â”€ reconstruction/        # Point cloud generation and registration
â”‚   â””â”€â”€ utils/                 # ICP, fusion, visualization, RealSense tools
â”‚
â”œâ”€â”€ installs/                     # Setup instructions and dependencies
â”œâ”€â”€ theoric/                      # Supplementary theoretical material
â”œâ”€â”€ main.py                       # Pipeline entry point
â””â”€â”€ README.md
```

---

## âš™ï¸ Installation

### 1. Clone this repository

```bash
git clone https://github.com/your-user/SIBGRAPI2025_slam.git
cd SIBGRAPI2025_slam
```

### 2. Clone DepthAnythingV2

```bash
git clone https://github.com/DepthAnything/Depth-Anything-V2
mv Depth-Anything-V2 Depth_Anything_V2
```

### 3. Create and activate virtual environment

```bash
python3 -m venv .venv
source .venv/bin/activate
```

### 4. Install dependencies

```bash
pip install --upgrade pip
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118  # cu121 cu128
pip install -r Depth_Anything_V2/requirements.txt
pip install open3d opencv-python numpy "numpy<2" pandas
```

---

## ğŸš€ Execution

Each stage of the pipeline is callable via functions defined in `main.py`.

### Available Stages

| Stage | Description                                                |
| ----- | ---------------------------------------------------------- |
| 1     | RealSense data capture + reconstruction using real depth   |
| 2     | Monocular depth inference + reconstruction                 |
| 3     | ICP alignment + depth map fusion + final 3D reconstruction |
| 4     | Trajectory visualization                                   |
| 5     | Side-by-side point cloud comparison                        |

### Example:

```python
scene = "lab_scene_kinect_xyz"
stage = 1  # or 2, 3, 4, 5
main()
```

---

## âœ… Expected Results

* Accurate 3D point cloud reconstructions
* Fused depth maps for completeness
* Trajectory visualization of estimated camera motion
* Visual and quantitative comparison between real and monocular depth

---

## ğŸ“Œ Notes

* No ROS2 is required; the system is ready for extension to OctoMap or RViz if needed.
* Depth inference runs offline on `.png` images.
* CUDA-compatible GPU is recommended for efficient processing.

---

## ğŸ‘¨â€ğŸ”¬ Author

Developed as part of a research project for **SIBGRAPI 2025**.

> Academic work under the graduate course **INF791 â€” Computer Vision for 3D Data**
